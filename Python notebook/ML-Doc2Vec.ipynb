{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook is for training different ML models on Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from molearn.classifiers.BR import BR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from molearn.classifiers.Ensemble import Ensemble\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from molearn.classifiers.classifier_chains import CC,RCC,MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some useful methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Hamming_loss(Ytest,Ypred):\n",
    "    ''' Hamming loss aka Hamming distance '''\n",
    "    return 1.-Hamming_score(Ytest,Ypred)\n",
    "\n",
    "def Hamming_score(Ytest,Ypred):\n",
    "    ''' Hamming score aka Hamming match '''\n",
    "    N_test,L = Ytest.shape\n",
    "    return np.sum((Ytest == Ypred) * 1.) / N_test / L\n",
    "\n",
    "def Hamming_matches(Ytest,Ypred):\n",
    "    N_test,L = Ytest.shape\n",
    "    return np.sum((Ytest == Ypred) * 1.,axis=0) / N_test \n",
    "\n",
    "def Hamming_losses(Ytest,Ypred):\n",
    "    return 1.-Hamming_matches(Ytest,Ypred)\n",
    "\n",
    "def Exact_match(Ytest,Ypred):\n",
    "    N_test,L = Ytest.shape\n",
    "    return np.sum(np.sum((Ytest == Ypred) * 1,axis=1)==L) * 1. / N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(train_x, train_y, test_x, test_y, parameters, pipeline):\n",
    "    grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=-1, verbose=10)\n",
    "    grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "    print\n",
    "    print(\"Best parameters set:\")\n",
    "    print (grid_search_tune.best_estimator_.steps)\n",
    "    print\n",
    "\n",
    "    # measuring performance on test set\n",
    "    print (\"Applying best classifier on test data:\")\n",
    "    best_clf = grid_search_tune.best_estimator_\n",
    "    \n",
    "    predictions = best_clf.predict(test_x)\n",
    "    print('grid_search_tune.best_estimator : ',grid_search_tune.best_estimator_.steps[0])\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save model\n",
    "def save_model(clf,filename,folder):\n",
    "    pickle.dump(clf, open(\"/home/sina/trained-model/\"+folder+\"/\"+filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logisticRegression(x_train, x_test, y_train, y_test):\n",
    "        \n",
    "    print (\"LogisticRegression\")\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        \"clf__estimator__C\": [0.1,1,10],\n",
    "        \"clf__estimator__class_weight\": [None],\n",
    "    }\n",
    "    clf = grid_search(x_train, y_train, x_test, y_test, parameters, pipeline)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaboost(x_train, x_test, y_train, y_test):\n",
    "        \n",
    "    print (\"LogisticRegression\")\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2),\n",
    "    algorithm=\"SAMME\"), n_jobs=-1)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        \"clf__estimator__learning_rate\": [1,1.5],\n",
    "        \"clf__estimator__n_estimators\": [6],\n",
    "    }\n",
    "    clf = grid_search(x_train, y_train, x_test, y_test, parameters, pipeline)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naivebayes(x_train, x_test, y_train, y_test):\n",
    "        \n",
    "    print (\"LogisticRegression\")\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None), n_jobs=-1)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "    }\n",
    "    clf = grid_search(x_train, y_train, x_test, y_test, parameters, pipeline)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm(x_train, x_test, y_train, y_test):\n",
    "\n",
    "\n",
    "    print (\"LinearSVC\")\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(SVC(), n_jobs=-1)),\n",
    "    ])\n",
    "    \n",
    "    parameters ={\n",
    "         'clf__estimator__kernel': ['rbf'],\n",
    "         'clf__estimator__gamma': [1e-3],\n",
    "         'clf__estimator__C': [10]\n",
    "        }\n",
    "        \n",
    "    clf = grid_search(x_train, y_train, x_test, y_test, parameters, pipeline)    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (100000, 300)\n",
      "y_train shape:  (100000, 28)\n",
      "x_test shape:  (40000, 300)\n",
      "y_test shape:  (40000, 28)\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.read_csv('/home/sina/input/Doc2Vec/x_data_train_doc2vec.csv')\n",
    "y_train = pd.read_csv('/home/sina/input/Doc2Vec/y_data_train_doc2vec.csv')\n",
    "x_test = pd.read_csv('/home/sina/input/Doc2Vec/x_data_test_doc2vec.csv')\n",
    "y_test = pd.read_csv('/home/sina/input/Doc2Vec/y_data_test_doc2vec.csv')\n",
    "x_train = shuffle(x_train.as_matrix())\n",
    "y_train = shuffle(y_train.as_matrix())\n",
    "x_test = shuffle(x_test.as_matrix())\n",
    "y_test = shuffle(y_test.as_matrix())\n",
    "x_train = x_train[0:100000]\n",
    "y_train = y_train[0:100000]\n",
    "x_test = x_test[0:40000]\n",
    "y_test = y_test[0:40000]\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 shape of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (100000, 300)\n",
      "y_train shape:  (100000, 28)\n",
      "x_test shape:  (40000, 300)\n",
      "y_test shape:  (40000, 28)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape: ', x_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 apply different algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4.1.1 OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] clf__estimator__C=0.1, clf__estimator__class_weight=None ........\n",
      "[CV] clf__estimator__C=0.1, clf__estimator__class_weight=None ........\n",
      "[CV] clf__estimator__C=1, clf__estimator__class_weight=None ..........\n",
      "[CV] clf__estimator__C=1, clf__estimator__class_weight=None ..........\n",
      "[CV] clf__estimator__C=10, clf__estimator__class_weight=None .........\n",
      "[CV] clf__estimator__C=10, clf__estimator__class_weight=None .........\n",
      "[CV]  clf__estimator__C=0.1, clf__estimator__class_weight=None, score=0.00324, total= 5.4min\n",
      "[CV]  clf__estimator__C=10, clf__estimator__class_weight=None, score=0.00328, total= 5.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   6 | elapsed:  5.9min remaining: 11.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=10, clf__estimator__class_weight=None, score=0.00236, total= 6.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:  6.0min remaining:  6.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=0.1, clf__estimator__class_weight=None, score=0.0024, total= 6.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   6 | elapsed:  6.2min remaining:  3.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__class_weight=None, score=0.00328, total= 6.2min\n",
      "[CV]  clf__estimator__C=1, clf__estimator__class_weight=None, score=0.00236, total= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  6.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('clf', OneVsRestClassifier(estimator=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "          n_jobs=-1))]\n",
      "Applying best classifier on test data:\n",
      "grid_search_tune.best_estimator :  ('clf', OneVsRestClassifier(estimator=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "          n_jobs=-1))\n"
     ]
    }
   ],
   "source": [
    "bst_clf_logisticregression_OneVsRestClassifier_Doc2Vec=logisticRegression(x_train, x_test, y_train, y_test)\n",
    "save_model(bst_clf_logisticregression_OneVsRestClassifier_Doc2Vec,'logisticregression_OneVsRestClassifier_Doc2Vec.sav','Doc2Vec')\n",
    "prediction_logisticregression_OneVsRestClassifier_Doc2Vec = bst_clf_logisticregression_OneVsRestClassifier_Doc2Vec.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4.1.2 ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 chain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 chain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=6)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=7)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 chain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        cv=None, order='random', random_state=9)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains = [ClassifierChain(LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
    "          penalty='l2', solver='sag', tol=0.0001,\n",
    "          verbose=0, warm_start=False), order='random', random_state=i)\n",
    "          for i in range(10)]\n",
    "\n",
    "for idx,chain in enumerate(chains):\n",
    "    if idx%4==0 or idx==10:\n",
    "        print(idx,\"chain\")\n",
    "    chain.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_chains = np.array([chain.predict(x_test) for chain in\n",
    "                          chains])\n",
    "predict_logisticregression_ensemble_ClassifierChain_Doc2Vec = y_pred_chains.mean(axis=0)\n",
    "\n",
    "temp = []\n",
    "org = []\n",
    "for i in range(len(predict_logisticregression_ensemble_ClassifierChain_Doc2Vec)):\n",
    "    for j in range(len(predict_logisticregression_ensemble_ClassifierChain_Doc2Vec[0])):\n",
    "        temp.append(int(predict_logisticregression_ensemble_ClassifierChain_Doc2Vec[i][j]+0.8))\n",
    "    org.append(temp)\n",
    "    temp = []\n",
    "    \n",
    "predict_logisticregression_ensemble_ClassifierChain_Doc2Vec = np.asarray(org) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<molearn.classifiers.classifier_chains.CC at 0x7f0bb2445ba8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = CC(h=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
    "          penalty='l2', solver='sag', tol=0.0001,\n",
    "          verbose=0, warm_start=False))\n",
    "cc.fit(x_train,y_train)\n",
    "save_model(cc,'LogisticRegression_Doc2Vec_CC.sav','Doc2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_logisticregression_CC_ClassifierChain_Doc2Vec=cc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<molearn.classifiers.BR.BR at 0x7f0bb21a5400>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = BR(h=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
    "          penalty='l2', solver='sag', tol=0.0001,\n",
    "          verbose=0, warm_start=False))\n",
    "br.fit(x_train,y_train)\n",
    "save_model(br,'LogisticRegression_Doc2Vec_BR.sav','Doc2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_logisticregression_BR_ClassifierChain_Doc2Vec=br.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<molearn.classifiers.classifier_chains.MCC at 0x7f0ba689ab00>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcc = MCC(h=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
    "          penalty='l2', solver='sag', tol=0.0001,\n",
    "          verbose=0, warm_start=False))\n",
    "mcc.fit(x_train,y_train)\n",
    "save_model(mcc,'LogisticRegression_Doc2Vec_MCC.sav','Doc2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_logisticregression_MCC_ClassifierChain_Doc2Vec=mcc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4.1.3 visualization and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 28)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_logisticregression_ensemble_ClassifierChain_Doc2Vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score_prediction_logisticregression_OneVsRestClassifier_Doc2Vec:  4.57596568791e-05\n",
      "f1_score_prediction_logisticregression_pure_chain_Doc2Vec:  0.220237363199\n",
      "f1_score_logisticregression_CC_ClassifierChain_Doc2Vec:  0.161522908015\n",
      "f1_score_logisticregression_BR_ClassifierChain_Doc2Vec:  0.059404578538\n",
      "f1_score_logisticregression_MCC_ClassifierChain_Doc2Vec:  0.142731164064\n"
     ]
    }
   ],
   "source": [
    "f1_score_prediction_logisticregression_OneVsRestClassifier_Doc2Vec = f1_score(y_test,prediction_logisticregression_OneVsRestClassifier_Doc2Vec,average='weighted')\n",
    "f1_score_prediction_logisticregression_pure_ClassifierChain_Doc2Vec = f1_score(y_test,predict_logisticregression_ensemble_ClassifierChain_Doc2Vec,average='weighted')\n",
    "f1_score_logisticregression_CC_ClassifierChain_Doc2Vec = f1_score(y_test,predict_logisticregression_CC_ClassifierChain_Doc2Vec,average='weighted')\n",
    "f1_score_logisticregression_BR_ClassifierChain_Doc2Vec = f1_score(y_test,predict_logisticregression_BR_ClassifierChain_Doc2Vec,average='weighted')\n",
    "f1_score_logisticregression_MCC_ClassifierChain_Doc2Vec = f1_score(y_test,predict_logisticregression_MCC_ClassifierChain_Doc2Vec,average='weighted')\n",
    "\n",
    "print('f1_score_prediction_logisticregression_OneVsRestClassifier_Doc2Vec: ',f1_score_prediction_logisticregression_OneVsRestClassifier_Doc2Vec)\n",
    "print('f1_score_prediction_logisticregression_pure_chain_Doc2Vec: ',f1_score_prediction_logisticregression_pure_ClassifierChain_Doc2Vec)\n",
    "print('f1_score_logisticregression_CC_ClassifierChain_Doc2Vec: ',f1_score_logisticregression_CC_ClassifierChain_Doc2Vec)\n",
    "print('f1_score_logisticregression_BR_ClassifierChain_Doc2Vec: ',f1_score_logisticregression_BR_ClassifierChain_Doc2Vec)\n",
    "print('f1_score_logisticregression_MCC_ClassifierChain_Doc2Vec: ',f1_score_logisticregression_MCC_ClassifierChain_Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_loss_prediction_logisticregression_OneVsRestClassifier_Doc2Vec:  0.0780696428571\n",
      "Hamming_loss_prediction_logisticregression_pure_chain_Doc2Vec:  0.121964285714\n",
      "Hamming_loss_logisticregression_CC_ClassifierChain_Doc2Vec:  0.14205\n",
      "Hamming_loss_logisticregression_BR_ClassifierChain_Doc2Vec:  0.0839267857143\n",
      "Hamming_loss_logisticregression_MCC_ClassifierChain_Doc2Vec:  0.199630357143\n"
     ]
    }
   ],
   "source": [
    "Hamming_loss_prediction_logisticregression_OneVsRestClassifier_Doc2Vec = Hamming_loss(y_test,prediction_logisticregression_OneVsRestClassifier_Doc2Vec)\n",
    "Hamming_loss_prediction_logisticregression_pure_ClassifierChain_Doc2Vec = Hamming_loss(y_test,predict_logisticregression_ensemble_ClassifierChain_Doc2Vec)\n",
    "Hamming_loss_logisticregression_CC_ClassifierChain_Doc2Vec = Hamming_loss(y_test,predict_logisticregression_CC_ClassifierChain_Doc2Vec)\n",
    "Hamming_loss_logisticregression_BR_ClassifierChain_Doc2Vec = Hamming_loss(y_test,predict_logisticregression_BR_ClassifierChain_Doc2Vec)\n",
    "Hamming_loss_logisticregression_MCC_ClassifierChain_Doc2Vec = Hamming_loss(y_test,predict_logisticregression_MCC_ClassifierChain_Doc2Vec)\n",
    "\n",
    "print('Hamming_loss_prediction_logisticregression_OneVsRestClassifier_Doc2Vec: ',Hamming_loss_prediction_logisticregression_OneVsRestClassifier_Doc2Vec)\n",
    "print('Hamming_loss_prediction_logisticregression_pure_chain_Doc2Vec: ',Hamming_loss_prediction_logisticregression_pure_ClassifierChain_Doc2Vec)\n",
    "print('Hamming_loss_logisticregression_CC_ClassifierChain_Doc2Vec: ',Hamming_loss_logisticregression_CC_ClassifierChain_Doc2Vec)\n",
    "print('Hamming_loss_logisticregression_BR_ClassifierChain_Doc2Vec: ',Hamming_loss_logisticregression_BR_ClassifierChain_Doc2Vec)\n",
    "print('Hamming_loss_logisticregression_MCC_ClassifierChain_Doc2Vec: ',Hamming_loss_logisticregression_MCC_ClassifierChain_Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_score_prediction_logisticregression_OneVsRestClassifier_Doc2Vec:  0.921930357143\n",
      "Hamming_score_prediction_logisticregression_pure_chain_Doc2Vec:  0.878035714286\n",
      "Hamming_score_logisticregression_CC_ClassifierChain_Doc2Vec:  0.85795\n",
      "Hamming_score_logisticregression_BR_ClassifierChain_Doc2Vec:  0.916073214286\n",
      "Hamming_score_logisticregression_MCC_ClassifierChain_Doc2Vec:  0.800369642857\n"
     ]
    }
   ],
   "source": [
    "Hamming_score_prediction_logisticregression_OneVsRestClassifier_Doc2Vec = Hamming_score(y_test,prediction_logisticregression_OneVsRestClassifier_Doc2Vec)\n",
    "Hamming_score_prediction_logisticregression_pure_ClassifierChain_Doc2Vec = Hamming_score(y_test,predict_logisticregression_ensemble_ClassifierChain_Doc2Vec)\n",
    "Hamming_score_logisticregression_CC_ClassifierChain_Doc2Vec = Hamming_score(y_test,predict_logisticregression_CC_ClassifierChain_Doc2Vec)\n",
    "Hamming_score_logisticregression_BR_ClassifierChain_Doc2Vec = Hamming_score(y_test,predict_logisticregression_BR_ClassifierChain_Doc2Vec)\n",
    "Hamming_score_logisticregression_MCC_ClassifierChain_Doc2Vec = Hamming_score(y_test,predict_logisticregression_MCC_ClassifierChain_Doc2Vec)\n",
    "\n",
    "print('Hamming_score_prediction_logisticregression_OneVsRestClassifier_Doc2Vec: ',Hamming_score_prediction_logisticregression_OneVsRestClassifier_Doc2Vec)\n",
    "print('Hamming_score_prediction_logisticregression_pure_chain_Doc2Vec: ',Hamming_score_prediction_logisticregression_pure_ClassifierChain_Doc2Vec)\n",
    "print('Hamming_score_logisticregression_CC_ClassifierChain_Doc2Vec: ',Hamming_score_logisticregression_CC_ClassifierChain_Doc2Vec)\n",
    "print('Hamming_score_logisticregression_BR_ClassifierChain_Doc2Vec: ',Hamming_score_logisticregression_BR_ClassifierChain_Doc2Vec)\n",
    "print('Hamming_score_logisticregression_MCC_ClassifierChain_Doc2Vec: ',Hamming_score_logisticregression_MCC_ClassifierChain_Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact_match_prediction_logisticregression_OneVsRestClassifier_Doc2Vec:  0.0\n",
      "Exact_match_prediction_logisticregression_pure_chain_Doc2Vec:  0.0022\n",
      "Exact_match_logisticregression_CC_ClassifierChain_Doc2Vec:  0.01635\n",
      "Exact_match_logisticregression_BR_ClassifierChain_Doc2Vec:  0.0074\n",
      "Exact_match_logisticregression_MCC_ClassifierChain_Doc2Vec:  0.00295\n"
     ]
    }
   ],
   "source": [
    "Exact_match_prediction_logisticregression_OneVsRestClassifier_Doc2Vec = Exact_match(y_test,prediction_logisticregression_OneVsRestClassifier_Doc2Vec)\n",
    "Exact_match_prediction_logisticregression_pure_ClassifierChain_Doc2Vec = Exact_match(y_test,predict_logisticregression_ensemble_ClassifierChain_Doc2Vec)\n",
    "Exact_match_logisticregression_CC_ClassifierChain_Doc2Vec = Exact_match(y_test,predict_logisticregression_CC_ClassifierChain_Doc2Vec)\n",
    "Exact_match_logisticregression_BR_ClassifierChain_Doc2Vec = Exact_match(y_test,predict_logisticregression_BR_ClassifierChain_Doc2Vec)\n",
    "Exact_match_logisticregression_MCC_ClassifierChain_Doc2Vec = Exact_match(y_test,predict_logisticregression_MCC_ClassifierChain_Doc2Vec)\n",
    "\n",
    "print('Exact_match_prediction_logisticregression_OneVsRestClassifier_Doc2Vec: ',Exact_match_prediction_logisticregression_OneVsRestClassifier_Doc2Vec)\n",
    "print('Exact_match_prediction_logisticregression_pure_chain_Doc2Vec: ',Exact_match_prediction_logisticregression_pure_ClassifierChain_Doc2Vec)\n",
    "print('Exact_match_logisticregression_CC_ClassifierChain_Doc2Vec: ',Exact_match_logisticregression_CC_ClassifierChain_Doc2Vec)\n",
    "print('Exact_match_logisticregression_BR_ClassifierChain_Doc2Vec: ',Exact_match_logisticregression_BR_ClassifierChain_Doc2Vec)\n",
    "print('Exact_match_logisticregression_MCC_ClassifierChain_Doc2Vec: ',Exact_match_logisticregression_MCC_ClassifierChain_Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier_Doc2Vec = DecisionTreeClassifier(min_samples_split=2, random_state=1)\n",
    "DecisionTreeClassifier_Doc2Vec.fit(x_train,y_train)\n",
    "save_model(DecisionTreeClassifier_Doc2Vec,'DecisionTreeClassifier_Doc2Vec.sav','Doc2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_DecisionTreeClassifier_Doc2Vec = DecisionTreeClassifier_Doc2Vec.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score_prediction_DecisionTreeClassifier_Doc2Vec:  0.207377540685\n"
     ]
    }
   ],
   "source": [
    "f1_score_DecisionTreeClassifier_Doc2Vec = f1_score(y_test,predict_DecisionTreeClassifier_Doc2Vec,average='weighted')\n",
    "\n",
    "print('f1_score_prediction_DecisionTreeClassifier_Doc2Vec: ',f1_score_DecisionTreeClassifier_Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_loss_DecisionTreeClassifier_Doc2Vec:  0.120939285714\n"
     ]
    }
   ],
   "source": [
    "Hamming_loss_DecisionTreeClassifier_Doc2Vec = Hamming_loss(y_test,predict_DecisionTreeClassifier_Doc2Vec)\n",
    "\n",
    "print('Hamming_loss_DecisionTreeClassifier_Doc2Vec: ',Hamming_loss_DecisionTreeClassifier_Doc2Vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_score_DecisionTreeClassifier_Doc2Vec:  0.879060714286\n"
     ]
    }
   ],
   "source": [
    "Hamming_score_DecisionTreeClassifier_Doc2Vec = Hamming_score(y_test,predict_DecisionTreeClassifier_Doc2Vec)\n",
    "\n",
    "print('Hamming_score_DecisionTreeClassifier_Doc2Vec: ',Hamming_score_DecisionTreeClassifier_Doc2Vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact_match_DecisionTreeClassifier_Doc2Vec:  0.0266\n"
     ]
    }
   ],
   "source": [
    "Exact_match_DecisionTreeClassifier_Doc2Vec = Exact_match(y_test,predict_DecisionTreeClassifier_Doc2Vec)\n",
    "\n",
    "print('Exact_match_DecisionTreeClassifier_Doc2Vec: ',Exact_match_DecisionTreeClassifier_Doc2Vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## BPMLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# bp mll loss function\n",
    "# y_true, y_pred must be 2D tensors of shape (batch dimension, number of labels)\n",
    "# y_true must satisfy y_true[i][j] == 1 iff sample i has label j\n",
    "def bp_mll_loss(y_true, y_pred):\n",
    " \n",
    "    # get true and false labels\n",
    "    y_i = K.equal(y_true, K.ones_like(y_true))\n",
    "    y_i_bar = K.not_equal(y_true, K.ones_like(y_true))\n",
    "    \n",
    "    # cast to float as keras backend has no logical and\n",
    "    y_i = K.cast(y_i, dtype='float32')\n",
    "    y_i_bar = K.cast(y_i_bar, dtype='float32')\n",
    "\n",
    "    # get indices to check\n",
    "    truth_matrix = pairwise_and(y_i, y_i_bar)\n",
    "\n",
    "    # calculate all exp'd differences\n",
    "    sub_matrix = pairwise_sub(y_pred, y_pred)\n",
    "    exp_matrix = K.exp(-sub_matrix)\n",
    "\n",
    "    # check which differences to consider and sum them\n",
    "    sparse_matrix = exp_matrix * truth_matrix\n",
    "    sums = K.sum(sparse_matrix, axis=[1,2])\n",
    "\n",
    "    # get normalizing terms and apply them\n",
    "    y_i_sizes = K.sum(y_i, axis=1)\n",
    "    y_i_bar_sizes = K.sum(y_i_bar, axis=1)\n",
    "    normalizers = y_i_sizes * y_i_bar_sizes\n",
    "    results = sums / normalizers\n",
    "\n",
    "    # sum over samples\n",
    "    return K.sum(results)\n",
    "\n",
    "\n",
    "# compute pairwise differences between elements of the tensors a and b\n",
    "def pairwise_sub(a, b):\n",
    "    column = K.expand_dims(a, 2)\n",
    "    row = K.expand_dims(b, 1)\n",
    "    return column - row\n",
    "\n",
    "# compute pairwise logical and between elements of the tensors a and b\n",
    "def pairwise_and(a, b):\n",
    "    column = K.expand_dims(a, 2)\n",
    "    row = K.expand_dims(b, 1)\n",
    "    return K.minimum(column, row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = x_train.shape[0]\n",
    "dim_no = x_train.shape[1]\n",
    "class_no = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100000/100000 [==============================] - 19s 186us/step - loss: 22.8158\n",
      "Epoch 2/50\n",
      "100000/100000 [==============================] - 18s 181us/step - loss: 22.6544\n",
      "Epoch 3/50\n",
      "100000/100000 [==============================] - 19s 191us/step - loss: 22.6297\n",
      "Epoch 4/50\n",
      "100000/100000 [==============================] - 19s 187us/step - loss: 22.6141\n",
      "Epoch 5/50\n",
      "100000/100000 [==============================] - 19s 187us/step - loss: 22.6047\n",
      "Epoch 6/50\n",
      "100000/100000 [==============================] - 19s 189us/step - loss: 22.6004\n",
      "Epoch 7/50\n",
      "100000/100000 [==============================] - 18s 182us/step - loss: 22.5942\n",
      "Epoch 8/50\n",
      "100000/100000 [==============================] - 19s 188us/step - loss: 22.5895\n",
      "Epoch 9/50\n",
      "100000/100000 [==============================] - 18s 182us/step - loss: 22.5878\n",
      "Epoch 10/50\n",
      "100000/100000 [==============================] - 19s 191us/step - loss: 22.5849\n",
      "Epoch 11/50\n",
      "100000/100000 [==============================] - 19s 195us/step - loss: 22.5786\n",
      "Epoch 12/50\n",
      "100000/100000 [==============================] - 19s 185us/step - loss: 22.5732\n",
      "Epoch 13/50\n",
      "100000/100000 [==============================] - 18s 179us/step - loss: 22.5708\n",
      "Epoch 14/50\n",
      "100000/100000 [==============================] - 18s 181us/step - loss: 22.5699\n",
      "Epoch 15/50\n",
      "100000/100000 [==============================] - 19s 186us/step - loss: 22.5637\n",
      "Epoch 16/50\n",
      "100000/100000 [==============================] - 19s 186us/step - loss: 22.5603\n",
      "Epoch 17/50\n",
      "100000/100000 [==============================] - 19s 192us/step - loss: 22.5570\n",
      "Epoch 18/50\n",
      "100000/100000 [==============================] - 19s 185us/step - loss: 22.5518\n",
      "Epoch 19/50\n",
      "100000/100000 [==============================] - 18s 183us/step - loss: 22.5470\n",
      "Epoch 20/50\n",
      "100000/100000 [==============================] - 18s 185us/step - loss: 22.5433\n",
      "Epoch 21/50\n",
      "100000/100000 [==============================] - 18s 179us/step - loss: 22.5339\n",
      "Epoch 22/50\n",
      "100000/100000 [==============================] - 18s 183us/step - loss: 22.5351\n",
      "Epoch 23/50\n",
      "100000/100000 [==============================] - 18s 185us/step - loss: 22.5305\n",
      "Epoch 24/50\n",
      "100000/100000 [==============================] - 19s 192us/step - loss: 22.5219\n",
      "Epoch 25/50\n",
      "100000/100000 [==============================] - 19s 189us/step - loss: 22.5212\n",
      "Epoch 26/50\n",
      "100000/100000 [==============================] - 19s 192us/step - loss: 22.5145\n",
      "Epoch 27/50\n",
      "100000/100000 [==============================] - 19s 188us/step - loss: 22.5024\n",
      "Epoch 28/50\n",
      "100000/100000 [==============================] - 18s 175us/step - loss: 22.5017\n",
      "Epoch 29/50\n",
      "100000/100000 [==============================] - 19s 186us/step - loss: 22.4981\n",
      "Epoch 30/50\n",
      "100000/100000 [==============================] - 19s 190us/step - loss: 22.4870\n",
      "Epoch 31/50\n",
      "100000/100000 [==============================] - 19s 187us/step - loss: 22.4842\n",
      "Epoch 32/50\n",
      "100000/100000 [==============================] - 19s 187us/step - loss: 22.4700\n",
      "Epoch 33/50\n",
      "100000/100000 [==============================] - 20s 199us/step - loss: 22.4703\n",
      "Epoch 34/50\n",
      "100000/100000 [==============================] - 19s 191us/step - loss: 22.4573\n",
      "Epoch 35/50\n",
      "100000/100000 [==============================] - 19s 192us/step - loss: 22.4532\n",
      "Epoch 36/50\n",
      "100000/100000 [==============================] - 20s 195us/step - loss: 22.4578\n",
      "Epoch 37/50\n",
      "100000/100000 [==============================] - 19s 186us/step - loss: 22.4437\n",
      "Epoch 38/50\n",
      "100000/100000 [==============================] - 18s 180us/step - loss: 22.4281\n",
      "Epoch 39/50\n",
      "100000/100000 [==============================] - 19s 191us/step - loss: 22.4216\n",
      "Epoch 40/50\n",
      "100000/100000 [==============================] - 19s 191us/step - loss: 22.4155\n",
      "Epoch 41/50\n",
      "100000/100000 [==============================] - 18s 183us/step - loss: 22.4080\n",
      "Epoch 42/50\n",
      "100000/100000 [==============================] - 19s 189us/step - loss: 22.3979\n",
      "Epoch 43/50\n",
      "100000/100000 [==============================] - 19s 193us/step - loss: 22.3912\n",
      "Epoch 44/50\n",
      "100000/100000 [==============================] - 19s 186us/step - loss: 22.3854\n",
      "Epoch 45/50\n",
      "100000/100000 [==============================] - 19s 189us/step - loss: 22.3722\n",
      "Epoch 46/50\n",
      "100000/100000 [==============================] - 19s 187us/step - loss: 22.3657\n",
      "Epoch 47/50\n",
      "100000/100000 [==============================] - 19s 189us/step - loss: 22.3545\n",
      "Epoch 48/50\n",
      "100000/100000 [==============================] - 19s 192us/step - loss: 22.3504\n",
      "Epoch 49/50\n",
      "100000/100000 [==============================] - 19s 187us/step - loss: 22.3400\n",
      "Epoch 50/50\n",
      "100000/100000 [==============================] - 19s 188us/step - loss: 22.3299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f977e9ab358>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "# create simple mlp\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=dim_no, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dense(class_no, activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
    "model.compile(loss=bp_mll_loss, optimizer='adagrad', metrics=[])\n",
    "\n",
    "# train a few epochs\n",
    "model.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "(40000, 28)\n",
      "[3.1791953e-06 2.7951914e-06 2.6871163e-01 9.9998665e-01 5.5849114e-07\n",
      " 1.6256499e-07 1.0344197e-05 3.1594253e-01 5.4229391e-01 9.9999940e-01\n",
      " 2.2325020e-01 1.3683565e-06 2.4109389e-01 3.6745474e-03 7.5281230e-07\n",
      " 9.9998951e-01 4.2138898e-01 5.7031051e-03 1.6633892e-01 2.7855486e-02\n",
      " 3.8611558e-01 7.5701493e-05 3.3685689e-07 1.0000000e+00 1.4891527e-02\n",
      " 7.0054483e-07 6.7208472e-05 8.8137954e-02]\n",
      "[[1.3273411e-05 7.6335245e-06 6.4799851e-01 ... 1.6033935e-06\n",
      "  2.2616122e-04 1.7970707e-01]\n",
      " [4.1749777e-06 2.0389450e-06 2.7775377e-01 ... 1.0875337e-06\n",
      "  6.9740898e-05 7.9124309e-02]\n",
      " [3.1791953e-06 2.7951914e-06 2.6871163e-01 ... 7.0054483e-07\n",
      "  6.7208472e-05 8.8137954e-02]\n",
      " ...\n",
      " [5.0294384e-06 3.2310497e-06 6.4135981e-01 ... 5.4245908e-07\n",
      "  1.0316181e-04 1.4684361e-01]\n",
      " [1.0198415e-05 7.7914710e-06 3.6501291e-01 ... 3.0050001e-06\n",
      "  2.0624149e-04 1.4864203e-01]\n",
      " [7.2894146e-04 6.4270879e-04 2.5764889e-01 ... 2.3743801e-04\n",
      "  5.9386650e-03 2.4242853e-01]]\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] res\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       328\n",
      "          1       0.00      0.00      0.00       739\n",
      "          2       0.00      0.00      0.00      2593\n",
      "          3       0.24      0.99      0.38      9425\n",
      "          4       0.00      0.00      0.00       670\n",
      "          5       0.00      0.00      0.00       126\n",
      "          6       0.00      0.00      0.00       964\n",
      "          7       0.00      0.00      0.00      2741\n",
      "          8       0.00      0.00      0.00      3253\n",
      "          9       0.39      1.00      0.56     15683\n",
      "         10       0.00      0.00      0.00      2307\n",
      "         11       0.00      0.00      0.00       290\n",
      "         12       0.00      0.00      0.00      2494\n",
      "         13       0.00      0.00      0.00      1900\n",
      "         14       0.00      0.00      0.00       808\n",
      "         15       0.20      0.99      0.33      7994\n",
      "         16       0.00      0.00      0.00      3740\n",
      "         17       0.00      0.00      0.00      1738\n",
      "         18       0.00      0.00      0.00      1991\n",
      "         19       0.00      0.00      0.00      1999\n",
      "         20       0.00      0.00      0.00      3817\n",
      "         21       0.00      0.00      0.00       929\n",
      "         22       0.00      0.00      0.00        84\n",
      "         23       0.39      1.00      0.56     15436\n",
      "         24       0.00      0.00      0.00      1504\n",
      "         25       0.00      0.00      0.00       107\n",
      "         26       0.00      0.00      0.00      1630\n",
      "         27       0.00      0.00      0.00      1815\n",
      "\n",
      "avg / total       0.18      0.55      0.27     87105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bpmll_Word2Vec = model.predict(x_test)\n",
    "\n",
    "\n",
    "#report =classification_report(Y_test, bpmll_Word2Vec)\n",
    "print (len(bpmll_Word2Vec))\n",
    "\n",
    "print (bpmll_Word2Vec.shape)\n",
    "\n",
    "b = bpmll_Word2Vec[2][0]\n",
    "\n",
    "a = bpmll_Word2Vec\n",
    "\n",
    "res = []\n",
    "tmp = []\n",
    "for j in range(bpmll_Word2Vec.shape[0]):\n",
    "\tfor i in range(bpmll_Word2Vec.shape[1]):\n",
    "\t\ttmp.append(int(bpmll_Word2Vec[j][i]+0.01))\n",
    "\tres.append(tmp)\n",
    "\ttmp = []\t\n",
    "\n",
    "print(bpmll_Word2Vec[2])\n",
    "\n",
    "print(a)\n",
    "print(y_test[2])\n",
    "res = pd.DataFrame(res)\n",
    "res = res.as_matrix()\n",
    "print(res,'res')\n",
    "report =classification_report(y_test, res)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13350089285714284"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hamming_loss(y_test,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8664991071428572"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hamming_score(y_test,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000925"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exact_match(y_test,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27176459750501497"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,res,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
